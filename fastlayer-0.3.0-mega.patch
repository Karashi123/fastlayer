f --git a/core/hotpaths.py b/core/hotpaths.py
index 3a1b2c1..9f8c0ee 100644
--- a/core/hotpaths.py
+++ b/core/hotpaths.py
@@ -1,27 +1,78 @@
 from __future__ import annotations
 
-import os
-import json
-import time
-import logging
+import os, json, time, logging, importlib, threading
 from pathlib import Path
-from typing import Tuple, Optional, Dict, Any
+from typing import Tuple, Optional, Dict, Any, Callable
+from tempfile import NamedTemporaryFile
 
 import numpy as np
 try:
-    from numba import njit, prange, float64  # type: ignore
+    from numba import njit, prange, float64 as nb_f64, float32 as nb_f32  # type: ignore
     _HAVE_NUMBA = True
 except Exception:
     _HAVE_NUMBA = False
 
-# runtime flags (health check may toggle these)
-_DISABLE_CPP = False
+# runtime flags (health check may toggle these) — writes are lock-guarded
+_DISABLE_CPP = False
+_LAST_IMPL: str = "none"
+_state_lock = threading.RLock()
 
 log = logging.getLogger("fastlayer.hotpaths")
 if not log.handlers:
     h = logging.StreamHandler()
     fmt = logging.Formatter("[%(levelname)s] %(message)s")
     h.setFormatter(fmt)
     log.addHandler(h)
     log.setLevel(os.environ.get("FASTLAYER_LOGLEVEL", "WARNING"))
 
+def _set_disable_cpp(v: bool) -> None:
+    global _DISABLE_CPP
+    with _state_lock:
+        _DISABLE_CPP = v
+
+def _set_last_impl(v: str) -> None:
+    global _LAST_IMPL
+    with _state_lock:
+        _LAST_IMPL = v
+
 # -------- Public helpers --------
 
-def prepare_arrays(n: int) -> Tuple[np.ndarray, np.ndarray]:
-    """Create two 1D float64 arrays of length n filled with random values."""
-    a = np.random.rand(n).astype(np.float64, copy=False)
-    b = np.random.rand(n).astype(np.float64, copy=False)
+_MAX_AUTOTUNE_N = 1 << 24  # DoS防止の上限
+
+def prepare_arrays(n: int, dtype: np.dtype = np.float64) -> Tuple[np.ndarray, np.ndarray]:
+    """Create two 1D arrays of length n filled with random values."""
+    if n < 1 or n > _MAX_AUTOTUNE_N:
+        raise ValueError(f"n out of bounds: {n}")
+    a = np.random.rand(n).astype(dtype, copy=False)
+    b = np.random.rand(n).astype(dtype, copy=False)
     return a, b
 
 # -------- Baseline / hot paths --------
 
 def dot_numpy(a: np.ndarray, b: np.ndarray) -> float:
     """Dot product using NumPy (prefer BLAS path)."""
     return float(np.dot(a, b))
 
 if _HAVE_NUMBA:
-    @njit(float64(float64[:], float64[:]), nopython=True, cache=True, fastmath=True, parallel=True)
-    def _dot_numba_impl(a, b):
+    @njit(nb_f64(nb_f64[:], nb_f64[:]), nopython=True, cache=True, fastmath=True, parallel=True)
+    def _dot_numba_f64(a, b):
         s = 0.0
         for i in prange(a.size):
             s += a[i] * b[i]
         return s
+
+    @njit(nb_f32(nb_f32[:], nb_f32[:]), nopython=True, cache=True, fastmath=True, parallel=True)
+    def _dot_numba_f32(a, b):
+        s = nb_f32(0.0)
+        for i in prange(a.size):
+            s += a[i] * b[i]
+        return s
 
 def dot_numba(a: np.ndarray, b: np.ndarray) -> float:
     """Dot product using Numba njit (nopython, parallel)."""
     if not _HAVE_NUMBA:
         return dot_numpy(a, b)
-    a64 = np.ascontiguousarray(a, dtype=np.float64)
-    b64 = np.ascontiguousarray(b, dtype=np.float64)
-    return float(_dot_numba_impl(a64, b64))
+    if a.dtype == np.float32 and b.dtype == np.float32:
+        a32 = np.ascontiguousarray(a, dtype=np.float32)
+        b32 = np.ascontiguousarray(b, dtype=np.float32)
+        return float(_dot_numba_f32(a32, b32))
+    a64 = np.ascontiguousarray(a, dtype=np.float64)
+    b64 = np.ascontiguousarray(b, dtype=np.float64)
+    return float(_dot_numba_f64(a64, b64))
 
 # -------- Config & dispatch --------
 
@@ -38,11 +89,28 @@ def _env_str(name: str, default: str) -> str:
     v = os.environ.get(name)
     return v if v is not None and v.strip() else default
 
 def _cache_path() -> Path:
-    return Path(os.environ.get("FASTLAYER_CACHE", str(Path.home() / ".cache"))) / "fastlayer.json"
+    base = Path(os.environ.get("FASTLAYER_CACHE", str(Path.home() / ".cache")))
+    try:
+        base = base.expanduser().resolve()
+    except Exception:
+        base = Path.home().joinpath(".cache")
+    return base / "fastlayer.json"
 
 def _load_cached_config() -> Dict[str, Any]:
     path = _cache_path()
     if path.exists():
         try:
             return json.loads(path.read_text())
         except Exception:
             return {}
     return {}
 
 def _save_cached_config(cfg: Dict[str, Any]) -> None:
     path = _cache_path()
-    try:
-        path.parent.mkdir(parents=True, exist_ok=True)
-        path.write_text(json.dumps(cfg, indent=2))
-    except Exception as e:
-        log.debug(f"could not save cache config: {e}")
+    try:
+        path.parent.mkdir(parents=True, exist_ok=True)
+        with NamedTemporaryFile("w", delete=False, dir=str(path.parent)) as tmp:
+            json.dump(cfg, tmp, indent=2)
+            tmp.flush()
+            os.fsync(tmp.fileno())
+            tmp_path = Path(tmp.name)
+        tmp_path.replace(path)  # atomic save
+    except Exception as e:
+        log.debug(f"could not save cache config: {e}")
 
 def get_dispatch_config() -> dict:
     """Resolve dispatch configuration (defaults -> cache -> env)."""
@@ -67,6 +135,23 @@ def get_dispatch_config() -> dict:
         cfg["med"] = int(med_env)
     return cfg
 
+def _validate_1d_same_len(a: np.ndarray, b: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
+    if a.ndim != 1 or b.ndim != 1:
+        raise ValueError(f"dot expects 1D arrays, got a.ndim={a.ndim}, b.ndim={b.ndim}")
+    if a.size != b.size:
+        raise ValueError(f"size mismatch: a.size={a.size}, b.size={b.size}")
+    return a, b
+
+def _validate_finite(a: np.ndarray, b: np.ndarray) -> None:
+    if not (np.isfinite(a).all() and np.isfinite(b).all()):
+        raise ValueError("inputs contain NaN/Inf")
+
+def get_last_backend() -> str:
+    return _LAST_IMPL
+
 def dot(a: np.ndarray, b: np.ndarray) -> float:
     """Auto-select an implementation based on problem size and availability.
     Environment overrides:
@@ -74,9 +159,14 @@ def dot(a: np.ndarray, b: np.ndarray) -> float:
       DOT_SMALL=<int> (default 8000)
       DOT_MED=<int> (default 200000)
     """
-    cfg = get_dispatch_config()
+    global _DISABLE_CPP
+
+    _validate_1d_same_len(a, b)
+    _validate_finite(a, b)
+    cfg = get_dispatch_config()
     impl = cfg["impl"]
     n = int(a.size)
 
     if impl != "auto":
         chosen = impl
@@ -88,33 +178,47 @@ def dot(a: np.ndarray, b: np.ndarray) -> float:
             chosen = "cpp"
 
     if chosen == "numpy":
         log.debug(f"dot -> numpy (n={n})")
-        return dot_numpy(a, b)
+        _set_last_impl("numpy"); return dot_numpy(a, b)
 
     if chosen == "numba":
         log.debug(f"dot -> numba (n={n})")
-        return dot_numba(a, b)
+        _set_last_impl("numba"); return dot_numba(a, b)
 
     if chosen == "cpp":
-        if _DISABLE_CPP:
-            log.debug("cpp disabled by health check; falling back to numba")
-            return dot_numba(a, b)
+        if _DISABLE_CPP:
+            log.debug("cpp disabled; falling back to numba")
+            _set_last_impl("numba(fallback_cpp_disabled)")
+            return dot_numba(a, b)
         try:
-            import cpp_hot  # type: ignore
+            # secure import inside package namespace (fallback to legacy ext name)
+            try:
+                cpp_hot = importlib.import_module("fastlayer.cpp_hot")
+            except Exception:
+                cpp_hot = importlib.import_module("cpp_hot")
             a64 = np.ascontiguousarray(a, dtype=np.float64)
             b64 = np.ascontiguousarray(b, dtype=np.float64)
             log.debug(f"dot -> cpp (n={n})")
-            return float(cpp_hot.dot_cpp(a64, b64))
+            _set_last_impl("cpp"); return float(cpp_hot.dot_cpp(a64, b64))
         except Exception as e:
             log.debug(f"cpp not available ({e}); falling back to numba")
-            return dot_numba(a, b)
+            _set_disable_cpp(True)
+            _set_last_impl("numba(fallback_cpp_import_error)")
+            return dot_numba(a, b)
 
     # Fallback
     log.debug(f"unknown DOT_IMPL={impl}, falling back to numba")
-    return dot_numba(a, b)
+    _set_last_impl("numba(fallback_unknown_impl)"); return dot_numba(a, b)
 
 # -------- Warmup --------
 
-def warmup(sizes: Optional[Tuple[int, ...]] = (8192, 200_000), use_cpp: bool = True) -> None:
+def warmup(sizes: Optional[Tuple[int, ...]] = (8192, 200_000), use_cpp: bool = True) -> None:
     """Pre-compile JITs and prime caches to amortize first-call cost."""
-    a, b = prepare_arrays(sizes[0])
+    a, b = prepare_arrays(sizes[0], dtype=np.float64)
     _ = dot_numpy(a, b)
-    _ = dot_numba(a, b)  # triggers Numba compilation
+    _ = dot_numba(a, b)  # Numba f64
+    a32, b32 = prepare_arrays(sizes[0], dtype=np.float32)
+    _ = dot_numba(a32, b32)  # Numba f32
@@ -122,13 +226,20 @@ def warmup(sizes: Optional[Tuple[int, ...]] = (8192, 200_000), use_cpp: bool = True) -> None:
-    if use_cpp:
+    if use_cpp and not _DISABLE_CPP:
         try:
-            import cpp_hot  # type: ignore
-            a2, b2 = prepare_arrays(sizes[-1])
-            _ = cpp_hot.dot_cpp(a2.astype(np.float64), b2.astype(np.float64))
-        except Exception:
-            pass
+            try:
+                cpp_hot = importlib.import_module("fastlayer.cpp_hot")
+            except Exception:
+                cpp_hot = importlib.import_module("cpp_hot")
+            a2, b2 = prepare_arrays(sizes[-1], dtype=np.float64)
+            _ = cpp_hot.dot_cpp(a2, b2)
+        except Exception:
+            _set_disable_cpp(True)
 
     log.debug(f"warmup done for sizes={sizes} (cpp tried={use_cpp})")
 
 # -------- Health check --------
 
 def health_check() -> Dict[str, Any]:
     """Probe backends and mark unusable ones disabled."""
-    global _DISABLE_CPP, _HAVE_NUMBA
+    global _HAVE_NUMBA
     status = {"numpy": True, "numba": _HAVE_NUMBA, "cpp": False, "errors": {}}
-    # numpy probe
-    try:
-        a,b = prepare_arrays(1024)
+    a,b = prepare_arrays(1024, dtype=np.float64)
+    try:
         _ = dot_numpy(a,b)
     except Exception as e:
         status["numpy"] = False
         status["errors"]["numpy"] = str(e)
@@ -140,27 +251,50 @@ def health_check() -> Dict[str, Any]:
         except Exception as e:
             status["numba"] = False
             status["errors"]["numba"] = str(e)
-            _HAVE_NUMBA = False
+            with _state_lock:
+                _HAVE_NUMBA = False
 
     # cpp probe
     try:
-        import cpp_hot  # type: ignore
-        _ = cpp_hot.dot_cpp(a,b)
+        try:
+            cpp_hot = importlib.import_module("fastlayer.cpp_hot")
+        except Exception:
+            cpp_hot = importlib.import_module("cpp_hot")
+        if not hasattr(cpp_hot, "dot_cpp"):
+            raise AttributeError("fastlayer.cpp_hot.dot_cpp missing")
+        _ = cpp_hot.dot_cpp(a,b)
         status["cpp"] = True
     except Exception as e:
         status["cpp"] = False
         status["errors"]["cpp"] = str(e)
-        _DISABLE_CPP = True
+        _set_disable_cpp(True)
 
     log.debug(f"health_check: {status}")
     return status
 
 # -------- Autotuner --------
 
-def _time_ms(fn, a, b, iters: int = 3) -> float:
-    ts = []
-    for _ in range(iters):
-        t0 = time.perf_counter()
-        _ = fn(a,b)
-        ts.append((time.perf_counter() - t0) * 1000.0)
-    return float(sum(ts) / len(ts))
+def _median_ms(fn: Callable[[np.ndarray, np.ndarray], float], a: np.ndarray, b: np.ndarray, iters: int = 3) -> float:
+    ts = []
+    for _ in range(max(1, iters)):
+        t0 = time.perf_counter()
+        _ = fn(a, b)
+        ts.append((time.perf_counter() - t0) * 1000.0)
+    ts.sort()
+    mid = len(ts) // 2
+    return float(ts[mid]) if ts else 0.0
+
+def _warn_threads():
+    for k in ("OPENBLAS_NUM_THREADS","MKL_NUM_THREADS","NUMBA_NUM_THREADS","OMP_NUM_THREADS"):
+        if not os.environ.get(k):
+            log.debug(f"hint: consider setting {k} to avoid oversubscription")
 
-def autotune(save: bool = True) -> Dict[str, int]:
+def autotune(save: bool = True, iters_small: int = 3, iters_large: int = 3) -> Dict[str, int]:
     """Measure small grid to pick thresholds for this machine and cache them.
     Returns {"DOT_SMALL": v1, "DOT_MED": v2}.
     """
-    # ensure compiled
-    warmup()
+    _warn_threads()
+    warmup()
 
     sizes = [2_000, 4_000, 8_000, 16_000, 32_000, 64_000, 128_000, 256_000]
     small = 8000
@@ -169,13 +303,20 @@ def autotune(save: bool = True) -> Dict[str, int]:
     # Find numpy -> numba crossover
     crossover_small = None
     for n in sizes:
         a,b = prepare_arrays(n)
-        t_np = _time_ms(dot_numpy, a, b, iters=2)
-        t_nb = _time_ms(dot_numba, a, b, iters=2)
+        t_np = _median_ms(dot_numpy, a, b, iters=iters_small)
+        t_nb = _median_ms(dot_numba, a, b, iters=iters_small)
         if t_nb < t_np:
             crossover_small = n
             break
     if crossover_small is not None:
         small = max(2000, crossover_small // 2)
 
     # Find numba -> cpp crossover (optional)
     crossover_med = None
     try:
-        import cpp_hot  # type: ignore
+        try:
+            cpp_hot = importlib.import_module("fastlayer.cpp_hot")
+        except Exception:
+            cpp_hot = importlib.import_module("cpp_hot")
         for n in sizes[::-1]:
-            a,b = prepare_arrays(n)
-            t_nb = _time_ms(dot_numba, a, b, iters=2)
-            t_cpp = _time_ms(lambda x,y: float(cpp_hot.dot_cpp(x,y)), a, b, iters=2)
+            a,b = prepare_arrays(n)
+            t_nb = _median_ms(dot_numba, a, b, iters=iters_large)
+            t_cpp = _median_ms(lambda x,y: float(cpp_hot.dot_cpp(x,y)), a, b, iters=iters_large)
             if t_cpp < t_nb:
                 crossover_med = n
                 break
         if crossover_med is not None:
             med = max(50000, crossover_med)
     except Exception:
         pass
@@ -186,3 +327,5 @@ def autotune(save: bool = True) -> Dict[str, int]:
         cache.update({"small": cfg["DOT_SMALL"], "med": cfg["DOT_MED"]})
         _save_cached_config(cache)
     log.debug(f"autotune -> {cfg}")
     return cfg
+
+# get_last_backend() 追加済み（上）
diff --git a/core/memdb.py b/core/memdb.py
index 7b5a1fc..c3c8a8e 100644
--- a/core/memdb.py
+++ b/core/memdb.py
@@ -1,47 +1,108 @@
-from collections import OrderedDict, defaultdict
-import time
-from typing import Any, Dict, Callable, Optional
+from collections import OrderedDict, defaultdict
+import time, threading
+from typing import Any, Dict, Optional
 
 class L1LRU:
     def __init__(self, capacity: int = 50_000):
         self.cap = capacity
         self.od: "OrderedDict[Any, tuple[Any, float, int]]" = OrderedDict()
 
     def get(self, k: Any) -> Optional[Any]:
         if k in self.od:
-            v, ts, h = self.od.pop(k)
-            self.od[k] = (v, ts, h+1)
+            v, ts, h = self.od.pop(k)
+            self.od[k] = (v, time.monotonic(), h + 1)  # ts 更新 & hit++
             return v
         return None
 
-    def put(self, k: Any, v: Any, hits: int = 0) -> None:
-        if k in self.od:
-            self.od.pop(k)
+    def put(self, k: Any, v: Any, hits: Optional[int] = None) -> None:
+        if k in self.od:
+            _, _, old_h = self.od.pop(k)
+            h = old_h if hits is None else hits
         elif len(self.od) >= self.cap:
             self.od.popitem(last=False)
-        self.od[k] = (v, time.time(), hits)
+            h = 0 if hits is None else hits
+        else:
+            h = 0 if hits is None else hits
+        self.od[k] = (v, time.monotonic(), h)
+
+    def __len__(self) -> int:
+        return len(self.od)
 
 class L2Store:
     def __init__(self, data: Dict[Any, Any]):
         self.data = data
         self.hits = defaultdict(int)
 
     def get(self, k: Any) -> Optional[Any]:
         v = self.data.get(k)
         if v is not None:
             self.hits[k] += 1
         return v
 
 class MemDB:
     def __init__(self, l2_data: Dict[Any, Any], l1_cap: int = 50_000, promote_hits: int = 2):
         self.l1 = L1LRU(l1_cap)
         self.l2 = L2Store(l2_data)
         self.promote_hits = promote_hits
+        self.l1_hits = 0
+        self.l2_hits = 0
+        self._lock = threading.RLock()
 
     def get(self, k: Any) -> Optional[Any]:
-        v = self.l1.get(k)
-        if v is not None:
-            return v
-        v = self.l2.get(k)
-        if v is not None and self.l2.hits[k] >= self.promote_hits:
-            self.l1.put(k, v, hits=self.l2.hits[k])
-        return v
+        with self._lock:
+            v = self.l1.get(k)
+            if v is not None:
+                self.l1_hits += 1
+                return v
+            v = self.l2.get(k)
+            if v is not None:
+                self.l2_hits += 1
+                if self.l2.hits[k] >= self.promote_hits:
+                    self.l1.put(k, v, hits=self.l2.hits[k])
+            return v
 
     def put(self, k: Any, v: Any, write_through: bool = False) -> None:
-        self.l2.data[k] = v
-        if write_through:
-            self.l1.put(k, v)
-        else:
-            self.l2.hits[k] = max(self.l2.hits[k], 1)
+        with self._lock:
+            self.l2.data[k] = v
+            if write_through:
+                self.l1.put(k, v)
+            else:
+                self.l2.hits[k] = max(self.l2.hits[k], 1)
+
+    def stats(self) -> Dict[str, Any]:
+        with self._lock:
+            total = self.l1_hits + self.l2_hits
+            return {
+                "l1_hits": self.l1_hits,
+                "l2_hits": self.l2_hits,
+                "hit_ratio": (self.l1_hits / total) if total else 0.0,
+                "l1_size": len(self.l1),
+                "l2_size": len(self.l2.data),
+                "promote_hits": self.promote_hits,
+            }
+
+    def delete(self, k: Any) -> None:
+        with self._lock:
+            self.l1.od.pop(k, None)
+            self.l2.data.pop(k, None)
+            self.l2.hits.pop(k, None)
+
+    def clear(self) -> None:
+        with self._lock:
+            self.l1.od.clear(); self.l2.data.clear(); self.l2.hits.clear()
+            self.l1_hits = self.l2_hits = 0
diff --git a/core/profile_utils.py b/core/profile_utils.py
index 4d9b0e2..a7f1c3b 100644
--- a/core/profile_utils.py
+++ b/core/profile_utils.py
@@ -1,10 +1,76 @@
-import cProfile, pstats, io
-from contextlib import contextmanager
+import cProfile, pstats, io, os, sys
+from contextlib import contextmanager
+from typing import Optional, Literal, IO
+from pathlib import Path
+from tempfile import NamedTemporaryFile
 
-@contextmanager
-def profile(label: str = "profile"):
-    pr = cProfile.Profile()
-    pr.enable()
-    try:
-        yield
-    finally:
-        pr.disable()
-        s = io.StringIO()
-        ps = pstats.Stats(pr, stream=s).sort_stats(pstats.SortKey.CUMULATIVE)
-        ps.print_stats(30)
-        print(f"\n=== {label} ===\n{s.getvalue()}")
+SortKey = Literal["cumulative", "tottime", "calls", "ncalls", "time"]
+
+def _dump_stats_safe(ps: pstats.Stats, out_path: str) -> None:
+    p = Path(out_path).expanduser().resolve()
+    p.parent.mkdir(parents=True, exist_ok=True)
+    with NamedTemporaryFile("wb", delete=False, dir=str(p.parent)) as tmp:
+        ps.dump_stats(tmp.name)
+        tmp.flush(); os.fsync(tmp.fileno())
+        tmp_path = Path(tmp.name)
+    tmp_path.replace(p)
+
+@contextmanager
+def profile(
+    label: str = "profile",
+    sort: SortKey = "cumulative",
+    limit: int = 30,
+    stream: Optional[IO[str]] = None,
+    out_path: Optional[str] = None,
+    strip_dirs: bool = True,
+    enabled: Optional[bool] = None,
+):
+    """
+    CPUプロファイルを取得し要約を出力、必要なら .prof を安全に保存。
+    - 有効化: enabled=True または環境変数 FL_PROFILE=1/true/yes/on
+    - sort: cumulative|tottime|calls|ncalls|time
+    - limit: 表示件数
+    - stream: 出力先（デフォルト stderr）
+    - out_path: pstatsダンプ（原子的保存）
+    - strip_dirs: パス短縮
+    """
+    if enabled is None:
+        val = os.getenv("FL_PROFILE", "")
+        enabled = val.lower() in ("1", "true", "yes", "on")
+
+    if not enabled:
+        # プロファイル無効時は素通り
+        yield
+        return
+
+    pr = cProfile.Profile()
+    pr.enable()
+    try:
+        yield
+    finally:
+        pr.disable()
+        out = stream if stream is not None else sys.stderr
+        ps = pstats.Stats(pr, stream=out)
+        if strip_dirs:
+            ps.strip_dirs()
+        key = {
+            "cumulative": pstats.SortKey.CUMULATIVE,
+            "tottime": pstats.SortKey.TIME,
+            "time": pstats.SortKey.TIME,
+            "calls": pstats.SortKey.CALLS,
+            "ncalls": pstats.SortKey.CALLS,
+        }.get(sort, pstats.SortKey.CUMULATIVE)
+        ps.sort_stats(key)
+        print(f"\n=== {label} (sort={sort}, limit={limit}) ===", file=out)
+        ps.print_stats(limit)
+        if out_path:
+            try:
+                _dump_stats_safe(ps, out_path)
+                print(f"[profile] dumped stats -> {out_path}", file=out)
+            except Exception as e:
+                print(f"[profile] dump failed: {e}", file=out)
diff --git a/fastlayer/__init__.py b/fastlayer/__init__.py
index 1111111..2222222 100644
--- a/fastlayer/__init__.py
+++ b/fastlayer/__init__.py
@@
-from .core.memdb import MemDB
-from .core.hotpaths import dot_numpy, dot_numba, prepare_arrays
-from .core.profile_utils import profile
-
-__all__ = ["MemDB", "dot_numpy", "dot_numba", "prepare_arrays", "profile"]
+from .core.memdb import MemDB
+from .core.hotpaths import (
+    dot, dot_numpy, dot_numba,
+    warmup, autotune, health_check,
+    prepare_arrays, get_last_backend,
+)
+from .core.profile_utils import profile
+
+__all__ = [
+    "MemDB",
+    "dot", "dot_numpy", "dot_numba",
+    "warmup", "autotune", "health_check",
+    "prepare_arrays", "get_last_backend",
+    "profile",
+]
diff --git a/README.md b/README.md
index 3333333..4444444 100644
--- a/README.md
+++ b/README.md
@@
 ## Installation
 
 ### From AUR (Arch Linux / Manjaro)
 ```bash
 yay -S python-fastlayer-git
+```
 
-
----
-
-### 4. サンプルコード
-```markdown
-## Example
-
-```python
-from fastlayer import memDB, hotpaths
-import numpy as np
-
-# Create DB
-db = memDB()
-
-# Warmup hot data
-db.warmup({"X": [1,2,3], "Y": [4,5,6]})
-
-# Autotune dot product
-X = np.arange(1000, dtype=np.float32)
-Y = np.arange(1000, dtype=np.float32)
-res = hotpaths.autotune("dot", X, Y)
-print("Dot result:", res)
-
-# Health check
-print(db.health_check())
-
-###
-
-from fastlayer import memDB, hotpaths
-import numpy as np
-
-db = memDB()
-db.warmup({"X": [1,2,3], "Y": [4,5,6]})
-
-X = np.arange(1000, dtype=np.float32)
-Y = np.arange(1000, dtype=np.float32)
-res = hotpaths.autotune("dot", X, Y)
-print("内積:", res)
-
-print(db.health_check())  # ヘルスチェック結果を表示
+## Example (EN)
+```python
+from fastlayer import MemDB, dot, warmup, autotune, health_check, get_last_backend
+import numpy as np
+
+print("health:", health_check())
+warmup()
+print("tuned:", autotune())
+
+X = np.arange(1000, dtype=np.float64)
+Y = np.arange(1000, dtype=np.float64)
+print("dot:", dot(X, Y), "backend:", get_last_backend())
+
+db = MemDB(l2_data={"X": X, "Y": Y})
+print("L2→L1 warm:", db.get("X") is not None)
+print("stats:", db.stats())
+```
+
+## サンプル (JP)
+```python
+from fastlayer import MemDB, dot, warmup, autotune, health_check, get_last_backend
+import numpy as np
+
+print("ヘルス:", health_check())
+warmup()
+print("チューニング:", autotune())
+
+X = np.arange(1000, dtype=np.float64)
+Y = np.arange(1000, dtype=np.float64)
+print("内積:", dot(X, Y), "実装:", get_last_backend())
+
+db = MemDB(l2_data={"X": X, "Y": Y})
+print("ウォーム:", db.get("X") is not None)
+print("統計:", db.stats())
